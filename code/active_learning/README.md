# Active Learning for Object Detection Improvement

This workflow helps you identify and annotate problematic frames to improve your object detection model, which will in turn improve event detection.

## ğŸ¯ Strategy

The core insight: **Many event detection failures stem from noisy object detections**. By identifying and retraining on problematic frames, you can:

1. âœ… Reduce edge detections (birds detected at frame borders)
2. âœ… Eliminate spike/dip artifacts (false positives/negatives)
3. âœ… Improve detection consistency in high-density scenes
4. âœ… Better capture count transitions (critical for event detection)

## ğŸ“‹ Workflow

### Step 1: Identify Problematic Frames

Run the frame identifier to analyze all detection CSVs:

```bash
./venv/bin/python code/active_learning/identify_problem_frames.py
```

This will:
- Analyze all CSV files in `csv_detection_1fps/`
- Identify frames with:
  - **Edge detections**: Birds detected near frame borders (often false positives)
  - **Spike frames**: Sudden count increases that disappear (detection errors)
  - **Dip frames**: Sudden count decreases that recover (missed detections)
  - **High count frames**: Scenes with many birds (harder to detect accurately)
  - **Count transitions**: Where bird count changes (critical for events)
- Save results to `data/problematic_frames.json`

**Expected output:**
```
Analyzing 31 CSV files...
================================================================================
[1/31] Processing TRI3_20250628T000000_raw... âœ“ Found 45 problematic frames
[2/31] Processing TRI3_20250628T001002_raw... âœ“ Found 23 problematic frames
...

SUMMARY STATISTICS
================================================================================
Files analyzed: 31
Files with problems: 28

Problem types:
  Edge detections: 547 frames
  Spike frames: 89 frames
  Dip frames: 124 frames
  High count frames: 67 frames
  Count transitions: 234 frames

Total problematic frames: 1061
```

### Step 2: Extract Frames for Annotation

Extract the most problematic frames from videos:

```bash
# Basic usage (extracts up to 100 frames per type, diverse sampling)
./venv/bin/python code/active_learning/extract_frames.py \
    --video-dir video \
    --output-dir data/frames_for_annotation \
    --max-per-type 100 \
    --priority diverse
```

**Options:**
- `--max-per-type`: Maximum frames to extract per problem type (default: 100)
- `--priority`:
  - `diverse`: Spread selection across different videos (recommended)
  - `concentrated`: Focus on worst videos first

**Output structure:**
```
data/frames_for_annotation/
â”œâ”€â”€ annotation_manifest.json    # Complete metadata
â”œâ”€â”€ frames_list.csv            # Simple list for quick reference
â”œâ”€â”€ edge_detection/            # Frames with edge detections
â”‚   â”œâ”€â”€ TRI3_20250628T030002_s0124_edge_detection.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ spike/                     # Frames with spike artifacts
â”‚   â”œâ”€â”€ TRI3_20250628T030002_s0037_spike.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ dip/                       # Frames with dip artifacts
â”‚   â””â”€â”€ ...
â”œâ”€â”€ high_count/                # Frames with many birds
â”‚   â””â”€â”€ ...
â””â”€â”€ count_transition/          # Frames where count changes
    â””â”€â”€ ...
```

### Step 3: Annotate the Frames

Use your preferred annotation tool:

**Option A: Roboflow** (recommended if you're already using it)
1. Create a new "Active Learning Batch" project or add to existing
2. Upload frames from `data/frames_for_annotation/`
3. Annotate birds in each frame
4. Export in YOLOv8 format

**Option B: CVAT / Label Studio**
1. Import frames
2. Use bounding box tool to annotate birds
3. Export in YOLO format

**Annotation tips:**
- For edge detections: Pay special attention to partial birds at borders
- For high count frames: Be thorough - these are hardest cases
- For transitions: These frames are critical for event detection accuracy

### Step 4: Combine with Existing Dataset

Merge the new annotations with your existing training data:

```bash
# Assuming you exported from Roboflow/CVAT as 'active_learning_batch/'
cp -r active_learning_batch/images/* dataset/seabird_fish*/images/train/
cp -r active_learning_batch/labels/* dataset/seabird_fish*/labels/train/

# Update dataset YAML if needed
```

### Step 5: Retrain Object Detection Model

Use your existing training script with the augmented dataset:

```bash
./venv/bin/python code/model/train.py \
    --data dataset/your_updated_dataset.yaml \
    --epochs 50 \
    --batch 16 \
    --name model_with_active_learning
```

### Step 6: Evaluate Improvement

Run inference with new model and compare:

```bash
# Run new inference
./venv/bin/python code/model/run_inference.py \
    --model runs/detect/model_with_active_learning/weights/best.pt \
    --source video/

# Run event detection with new detections
./venv/bin/python code/postprocess/benchmark_state_based_detector.py
```

**Expected improvements:**
- Fewer edge detections â†’ Less false positive events
- Fewer spikes/dips â†’ Smoother count signals
- Better transition detection â†’ Higher event detection accuracy
- Overall: Event detection F1 should increase from 61% to potentially 70%+

## ğŸ“Š Monitoring Impact

Track these metrics before and after retraining:

### Object Detection Level:
- Number of edge detections per video
- Spike/dip frequency
- Detection consistency (frame-to-frame)

### Event Detection Level:
- Overall F1-score
- Departure detection F1 (biggest problem in current model)
- False positive rate
- False negative rate

## ğŸ’¡ Tips & Best Practices

1. **Start Small**: Begin with 50-100 frames per type to validate the workflow
2. **Focus on Failures**: Prioritize frame types causing the most event detection errors
3. **Iterate**: This is an iterative process - you may need 2-3 rounds
4. **Balance Dataset**: Don't oversample one frame type - maintain class balance
5. **Quality Over Quantity**: Better to annotate 100 frames well than 500 frames poorly

## ğŸ”„ Iterative Improvement Loop

```
1. Current Model â†’ 2. Identify Problems â†’ 3. Extract Frames â†’ 
4. Annotate â†’ 5. Retrain â†’ 6. Evaluate â†’ (back to 1)
```

Each iteration should improve:
- Object detection quality
- Event detection accuracy
- Overall system reliability

## ğŸ“ Expected Timeline

- **Analysis (Step 1)**: 5 minutes
- **Frame Extraction (Step 2)**: 10-15 minutes
- **Annotation (Step 3)**: 2-4 hours (for 500 frames)
- **Retraining (Step 5)**: 1-2 hours
- **Evaluation (Step 6)**: 30 minutes

**Total**: ~4-7 hours for one iteration

## ğŸ¯ Success Criteria

You'll know the active learning worked when:
- âœ… Edge detection count drops by 50%+
- âœ… Spike/dip frequency reduces significantly
- âœ… Event detection F1 improves by 5-10 percentage points
- âœ… Departure detection F1 increases substantially (currently at 58.5%)
- âœ… False positive rate decreases

## ğŸ†˜ Troubleshooting

**Problem**: Can't find video files
- **Solution**: Check `--video-dir` path, ensure videos have expected extensions (.mp4, .avi, etc.)

**Problem**: Too many frames to annotate
- **Solution**: Reduce `--max-per-type` or use `--priority concentrated` to focus on worst cases

**Problem**: Extraction is slow
- **Solution**: Videos are large - expect ~1-2 min per video. Consider extracting from subset first.

## ğŸ“š Related Files

- `identify_problem_frames.py`: Frame identification logic
- `extract_frames.py`: Frame extraction from videos
- `../postprocess/event_detector.py`: Event detection using these improved detections
- `../postprocess/state_based_detector.py`: State-based event detector that benefits from better detections

## ğŸš€ Future Enhancements

Potential improvements to this workflow:
- Uncertainty-based sampling (use model confidence scores)
- Hard negative mining (focus on false positives)
- Temporal consistency checks (multi-frame context)
- Automatic quality assessment of annotations
